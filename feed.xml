<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://bacnguyencong.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://bacnguyencong.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-22T16:23:59+00:00</updated><id>https://bacnguyencong.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Introduction to diffusion and flow models</title><link href="https://bacnguyencong.github.io/blog/2026/flow-diffusion-models/" rel="alternate" type="text/html" title="Introduction to diffusion and flow models"/><published>2026-02-06T15:12:00+00:00</published><updated>2026-02-06T15:12:00+00:00</updated><id>https://bacnguyencong.github.io/blog/2026/flow-diffusion-models</id><content type="html" xml:base="https://bacnguyencong.github.io/blog/2026/flow-diffusion-models/"><![CDATA[<blockquote> <p>This post covers the fundamentals of diffusion and flow matching models. The following notation and theoretical framework are primarily adapted from <a class="citation" href="#flowmodels2026">(Holderrieth &amp; Shprints, 2025)</a>.</p> </blockquote> <h1 id="1-background">1. Background</h1> <p>We begin with some mathematical definitions that help us to describe flow matching and diffusion models.</p> <p>A <strong>Trajectory</strong> is a function of form \(X\colon [0, 1] \to \mathbb{R}^d\) that maps from time $t$ to a vector in $\mathbb{R}^d$, i.e., $t \mapsto X_t$.</p> <p>A <strong>Vector field</strong> is defined as \(u\colon \mathbb{R}^d \times [0,1] \to \mathbb{R}^d\,,\) which maps from a location $x$ at time $t$ to a vector $u(x, t) \in \mathbb{R}^d$, indicating the velocity in the space, i.e., $(x, t) \mapsto u_t(x)$.</p> <p><strong>Ordinary differential equation (ODE)</strong> describes a condition on the trajectory. We want a trajectory $X_t$ to follow a line specified by the vector field $u_t(.)$. Given an initial condition, $X_0=x_0$, we define such a trajectory as the solution to the ODE equation</p> \[\begin{align*} \frac{dX_t}{dt} &amp;= u_t(X_t) \\ X_0 &amp;= x_0 \end{align*}\] <p><strong>Flow</strong> is a set of trajectories with different initial conditions. In other words, if we start at $X_0 = x_0$ and $t=0$, where we are at time $t$.</p> \[\begin{align*} \psi \colon \mathbb{R}^d \times [0, 1] &amp;\to \mathbb{R}^d \\ \psi_0(x_0) &amp;= x_0 \\ \frac{d}{dt} \psi_t(x_0) &amp;= u_t(\psi_t(x_0)) \end{align*}\] <p>For a given condition $X_0=x_0$, one can recover the trajectory via $X_t = \psi_t(x_0)$. Under the mild condition that $u$ is continuously differentiable with a bounded derivative, then the ODE has a unique solution given by a flow $\psi_t$. As a result, $\psi_t$ is a <em>diffeomorphism</em> for all $t$ (i.e., continuously differentiable with a continuously differentiable inverse $\psi_t^{-1}$).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ode-480.webp 480w,/assets/img/ode-800.webp 800w,/assets/img/ode-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ode.png" class="img-fluid rounded w-75 mx-auto d-block" width="100%" height="auto" title="Relationship between ODE, vector field, and flow" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"><b style="color: var(--global-theme-color)">Figure 1:</b> Relationship between ODE, vector field, and flow</figcaption> </figure> <p><strong>ODE solver:</strong> One of the simplest ways to solve an ODE is the <strong>Euler method</strong>. Starting with an initial condition $X_0=x_0$, we update the solution as</p> \[X_{t + h} = X_t + h \,. u_t(X_t) \,,\] <p>where $h=1/N$ is the step size.</p> <p>Another second order method is <strong>Heun’s method</strong>, which defines the update via</p> \[\begin{align*} X'_{t + h} &amp;= X_t + h \,. u_t(X_t) \\ X_{t + h} &amp;= X_t + \frac{h}{2} \,. \Big( u_t(X_t) + u_{t+h}(X'_{t + h})\Big) \end{align*}\] <p>Intuitively, it takes the first solution guess, then corrects the solution using the updated guess.</p> <h1 id="2-flow-and-diffusion-models">2. Flow and diffusion models</h1> <p>We aim to convert a base distribution $p_\mathrm{init}$ into a complex distribution $p_\mathrm{data}$. This allows us to generate samples from $p_\mathrm{data}$.</p> <h2 id="21-flow-models">2.1. Flow models</h2> <p>Consider an ODE to construct the transformation. A flow model is defined by an ODE</p> \[\begin{aligned} X_0 &amp;\sim p_\mathrm{init} \\ \frac{d}{dt} X_t &amp;= u_t^\theta (X_t) \end{aligned}\] <p>where the vector field is parameterized by a neural network $u^\theta$. The goal is to learn this vector field such that</p> \[\begin{aligned} X_1 \sim p_\mathrm{data} \Leftrightarrow \psi_1^\theta(X_0) \sim p_\mathrm{data}\,, \end{aligned}\] <p>where $\psi^\theta$ is the flow derived from vector field $u^\theta$. Once the flow model is trained, we can generate samples from $p_\mathrm{data}$ using any ODE solver.</p> <h2 id="22-diffusion-models">2.2. Diffusion models</h2> <p>Instead of using an ODE that defines deterministic trajectories, diffusion models use a stochastic differential equation (SDE) to define the transformation from $p_\mathrm{init}$ to $p_\mathrm{data}$. A stochastic trajectory is a stochastic process $X(t)$ and it is given by</p> \[\begin{align*} X_0 &amp;\sim p_\mathrm{init} \\ dX_t &amp;= u_t^\theta(X_t) dt + \sigma(t) dW \end{align*}\] <p>where $W$ is a Brownian motion, the function $\sigma\colon \mathbb{R} \to \mathbb{R}$ is called <em>diffusion coefficient</em> and the function $u_t^\theta\colon \mathbb{R}^d \times [0,1] \to \mathbb{R}^d$ is called <em>drift coefficient</em>. Every ODE is also an SDE with $\sigma(t)=0$. Note that $\sigma(t)$ is fixed and $X_t$ is a random variable for all $0 \le t \le 1$.</p> <p>A SDE can be solved using a simple method such as Euler-Maruyama, which updates</p> \[X_{t + h} = X_t + h\,. u_t^\theta (X_t) + \sqrt{h} \sigma(t) \epsilon\] <p>The above update rule looks very similar to the Euler method except the last element, where we add a Gaussian noise $\epsilon$ scaled by $\sqrt{h} \sigma(t)$.</p> <h1 id="3-flow-matching">3. Flow matching</h1> <p>As described above, both flow and diffusion models can be parameterized by a neural network $u_t^\theta(.)$ representing a vector field. We can obtain samples from this model by solving the ODE</p> \[X_0 \sim p_\mathrm{init}, \, dX_t = u_t^\theta(X_t)dt \,.\] <p>Our goal is to make $X_1 \sim p_\mathrm{data}$. So, the question is how do we train this vector field? This section will describe flow matching <a class="citation" href="#lipman2022flow">(Lipman et al., 2022; Liu et al., 2022)</a>, a technique to train $u_t^\theta$.</p> <h2 id="31-conditional-and-marginal-probability-path">3.1. Conditional and marginal probability path</h2> <p>Note that only two endpoints at $t=0$ and $t=1$ have to satisfy our conditions $p_0=p_\mathrm{init}$ and $p_1=p_\mathrm{data}$. We have some freedom to design probability distributions $p_t$ in between $0 &lt; t &lt; 1$. Changing the conditional field changes how you travel, but not where you start or end. While the distributions $p_0$ and $p_1$ don’t change, the pathway between them can change. In the following, we describe our design.</p> <p><strong>Conditional probability path</strong> is a set of distributions that gradually convert the initial distribution $p_\mathrm{init}$ into a Dirac delta distribution (i.e., a single point).</p> \[p_0(.|z) = p_\mathrm{init}, \, p_1(.|z) = \delta_z \quad \text{for all} \, z \in \mathbb{R}^d\] <p>Essentially, the conditional probability path can be considered as a trajectory in the space of distributions.</p> <p><strong>Probability path</strong> defines a set of distributions obtained by marginalizing the conditional probability path</p> \[p_t(x) = \int p_t(x|z) p_{\mathrm{data}}(z) dz \,.\] <p>It’s straightforward to see that the marginal probability path $p_t$ interpolates between $p_\mathrm{init}$ and $p_\mathrm{data}$.</p> \[p_0(.|z) = p_\mathrm{init}, \, p_1(.|z) = p_\mathrm{data}\,.\] <p>Although $p_t$ is intractable, we can sample from it as the conditional probability path is often tractable by design.</p> <blockquote class="block-tip"> <h5 id="example-gaussian-conditional-probability-path">Example: Gaussian conditional probability path</h5> <p>Let $\alpha_t, \beta_t$ denote two continuously differentiable, monotonic functions with $\alpha_0=\beta_1=0$ and $\alpha_1=\beta_0=1$. The Gaussian conditional probability path is defined as</p> \[p_t (.|z) = \mathcal{N}(\alpha_t z, \beta^2 I_d)\] <p>At the endpoints, we have</p> \[p_0(.|z) = \mathcal{N}(0, I_d) \quad \text{and} \quad p_1(.|z) = \mathcal{N}(z, 0) = \delta_z\] <p>Sampling from the marginal $p_t$ can be expressed as</p> \[z\sim p_\mathrm{data}, \epsilon \sim p_\mathrm{init} \rightarrow x = \alpha_t z + \beta_t \epsilon \,.\] </blockquote> <h2 id="32-conditional-and-marginal-vector-field">3.2. Conditional and marginal vector field</h2> <p>So far, we have designed a marginal probability path $p_t$ that the points $X_t$ along a trajectory should have. The remaining task is to find a vector field such that trajectories $X_t$ follow the probability path.</p> <p><strong>Conditional vector field</strong> is a vector field $u^\mathrm{target}_t(.\vert z)$ such that the corresponding ODE yields the conditional probability path \(p_t(.\vert z)\), i.e.,</p> \[\frac{d}{dt} X_t = u^\mathrm{target}_t(.| z) \,, X_0 \sim p_\mathrm{init} \rightarrow X_t \sim p_t(.|z)\] <p>The conditional vector field can be derived analytically by hand.</p> <blockquote class="block-tip"> <h5 id="example-conditional-gaussian-vector-field">Example: Conditional Gaussian vector field</h5> <p>Let $p(. \vert z)=\mathcal{N}(\alpha_t z, \beta_t^2 I_d)$, the conditional Gaussian vector field is given by as</p> \[u^\mathrm{target}_t (x | z) = \left( \dot{\alpha}_t - \frac{\dot{\beta}_t}{\beta_t} \alpha_t \right) z + \frac{\dot{\beta}_t}{\beta_t} x\] </blockquote> <p>Given a conditional vector field $u^\mathrm{target}_t(.\vert z)$, the <strong>marginal vector field</strong> $u^\mathrm{target}_t(x)$ can be expressed as</p> \[\begin{equation} u^\mathrm{target}_t(x) = \int u^\mathrm{target}_t(x \vert z) \frac{p_t(x|z) p_\mathrm{data}(z)}{p_t (x)} dz \label{eq:marginal_vector_field} \end{equation}\] <p>The marginal vector field follows the marginal probability path</p> \[X_0 \sim p_\mathrm{init}, \, \frac{d}{dt} X_t = u^\mathrm{target}_t(X_t) \rightarrow X_t \sim p_t(.) \quad (0 \le t \le 1)\] <h2 id="33-how-to-train-the-marginal-vector-field">3.3. How to train the marginal vector field?</h2> <p>Equation $\eqref{eq:marginal_vector_field}$ provides an intuitive way to learn the marginal vector field $u^\mathrm{target}_t(X_t)$, e.g., using mean-squared error</p> \[\mathcal{L}_\mathrm{FM} (\theta) = \mathbb{E}_{t \sim \mathcal{U}(0,1), z\sim p_\mathrm{data}, x \sim p_t(x|z)} [ \|u^\mathrm{target}_t(x) - u^\theta_t(x) \|^2]\] <p>We first draw a random time $t\in [0,1]$, then a data point $z$ from our data set. Finally, we sample $x$ from the conditional distribution and compute $u_t^\theta$. While the formula to compute $u^\mathrm{target}_t(x)$ is known, we cannot compute it efficiently. Instead, we use the conditional flow matching to learn the vector field as</p> \[\mathcal{L}_\mathrm{CFM} (\theta) = \mathbb{E}_{t \sim \mathcal{U}(0,1), z\sim p_\mathrm{data}, x \sim p_t(x|z)} [ \|u^\mathrm{target}_t(x| z) - u^\theta_t(x) \|^2]\] <p>As the conditional vector field is tractable, one can minimize the above loss easily. Although the target vector field is not the same, the marginal flow matching loss equals the conditional flow matching up to a constant, i.e.,</p> \[\mathcal{L}_\mathrm{FM}(\theta) = \mathcal{L}_\mathrm{CFM}(\theta) + C\] <p>Therefore, optimizing the conditional flow matching loss is equivalent to minimizing the flow matching loss.</p> <blockquote class="block-tip"> <h5 id="example-flow-matching-for-gaussian-conditional-probability-path">Example: Flow matching for Gaussian conditional probability path</h5> <p>Let consider the straight-path (Optimal Transport) case $\alpha_t = t$ and $\beta_t = 1 - t$, then we have</p> \[u^\mathrm{target}_t (x | z) = z - \epsilon \,.\] <p>Therefore, the objective function becomes</p> \[\mathcal{L}_\mathrm{CFM} (\theta) = \mathbb{E}_{t \sim \mathcal{U}(0,1), z\sim p_\mathrm{data}, \epsilon \sim \mathcal{N}(0, I_d)} [ \|u^\mathrm{target}_t(tz + (1 -t)\epsilon) - (z - \epsilon) \|^2]\] </blockquote> <h1 id="4-score-matching">4. Score matching</h1> <p>Unlike flow models, diffusion models <a class="citation" href="#sohl2015deep">(Sohl-Dickstein et al., 2015; Song et al., 2020)</a> use SDEs to define the transformation from $p_\mathrm{init}$ to $p_\mathrm{data}$. This section describes diffusion models and how to train them using score matching.</p> <h2 id="41-conditional-and-marginal-score-functions">4.1. Conditional and marginal score functions</h2> <p><strong>Conditional score function</strong> is defined as $\nabla_x \log p_t(x \vert z)$ i.e., the gradient of the log-likelihood of the conditional density $p_t(x \vert z)$ with respect to $x$.</p> <p><strong>Marginal score function</strong> can be derived as</p> \[\nabla_x \log p_t(x) = \frac{\nabla_x p_t(x)}{p_t(x)} = \int \nabla_x \log p_t(x | z) \frac{p_t(x|z)p_\mathrm{data}(z)}{p_t(x)} dz \,.\] <p>The result looks very similar to the relationship between the conditional and marginal vector fields.</p> <blockquote class="block-tip"> <h5 id="example-score-function-for-gaussian-probability-path">Example: Score function for Gaussian probability path</h5> <p>For the Gaussian probability path $p_t(x \vert z) = \mathcal{N}(\alpha_t z, \beta_t^2 I_d)$, the conditional score function is defined as</p> \[\nabla_x \log p_t(x | z) = - \frac{x - \alpha_t z}{\beta_t^2}\] <p>Note that the score function for the Gaussian path is a linear combination of $x$ and $z$. As a result, the conditional (marginal) vector field can be recovered from the conditional (marginal) score function.</p> </blockquote> <p>For any diffusion coefficient $\sigma_t \ge 0$, one can construct an SDE as follows:</p> \[\begin{align} dX_t = \left [u^{\mathrm{target}}(X_t) + \frac{\sigma_t^2}{2} \nabla_x \log p_t (X_t) \right ] dt + \sigma_t dW_t \,. \label{eq:sde_diffusion} \end{align}\] <p>The marginal distribution $p_t$ will be the same as in flow models, i.e., $X_t \sim p_t$. Now the trajectories are stochastic due to the nature of the SDE’s evolution. Although Equation \eqref{eq:sde_diffusion} holds for an arbitrary choice of $\sigma_t$, in practice one must carefully choose $\sigma_t$, which must be empirically determined.</p> <blockquote class="block-tip"> <h5 id="example-sde-for-gaussian-probability-path">Example: SDE for Gaussian probability path</h5> <p>For the Gaussian probability path $p_t(x \vert z) = \mathcal{N}(\alpha_t z, \beta_t^2 I_d)$, we don’t need to train the vector field $u^\theta_t$ and score function $s^\theta_t$ separately. We can simulate the SDE as</p> \[dX_t = \left[ \left(a_t + \frac{\sigma_t^2}{2} s_t^\theta(X_t) + b_t Xt \right) \right] dt + \sigma_t dW_t\] <p>where</p> \[a_t = \left( \beta_t^2 \frac{\dot{\alpha}_t}{\alpha_t} - \dot{\beta}_t\beta_t \right)\,, b_t = \frac{\dot{\alpha}_t}{\alpha_t}\] </blockquote> <h2 id="42-how-to-train-score-function">4.2. How to train score function?</h2> <p>Similarly to the marginal vector field, we can learn the score function $s^\theta_t(x)$ using the score matching loss and denoising score matching loss</p> \[\begin{align*} \mathcal{L}_\mathrm{SM}(\theta) &amp;= \mathbb{E}_{t \sim \mathcal{U}(0,1), z\sim p_\mathrm{data}, x \sim p_t(x|z)} [ \|\nabla_x \log p_t(x) - s^\theta_t(x) \|^2] \\ \mathcal{L}_\mathrm{CSM}(\theta) &amp;= \mathbb{E}_{t \sim \mathcal{U}(0,1), z\sim p_\mathrm{data}, x \sim p_t(x|z)} [ \|\nabla_x \log p_t(x| z) - s^\theta_t(x) \|^2] \end{align*}\] <p>Although the target for score function is not the same, the score matching loss is equal to the denoising score matching loss up to a constant.</p> \[\mathcal{L}_\mathrm{SM}(\theta) = \mathcal{L}_\mathrm{DSM}(\theta) + C \,.\] <blockquote class="block-tip"> <h5 id="example-score-matching-for-gaussian-probability-path">Example: Score matching for Gaussian probability path</h5> <p>For the Gaussian probability path $p_t(x \vert z) = \mathcal{N}(\alpha_t z, \beta_t^2 I_d)$, the denoising score matching loss becomes</p> \[\mathcal{L}_\mathrm{CSM}(\theta) = \mathbb{E}_{t \sim \mathcal{U}(0,1), z\sim p_\mathrm{data}, x \sim p_t(x|z)} \left[ \left\|\frac{\epsilon}{\beta_t} + s^\theta_t(\alpha_t z + \beta_t \epsilon) \right \|^2 \right]\] <p>To avoid numerical instability for $\beta_t\approx 0$, we can drop $1/\beta_t$ in the loss and reparameterize $s^\theta$ into a noise predictor network $\epsilon^\theta$ <a class="citation" href="#ho2020denoising">(Ho et al., 2020)</a>, i.e.,</p> \[\mathcal{L}_\mathrm{DDPM}(\theta) = \mathbb{E}_{t \sim \mathcal{U}(0,1), z\sim p_\mathrm{data}, x \sim p_t(x|z)} \left[ \left\|\epsilon - \epsilon^\theta_t(\alpha_t z + \beta_t \epsilon) \right \|^2 \right]\] <p>Note that one can recover the score as $s_t^\theta(x) = -\epsilon_t^\theta(x)/\beta_t$.</p> </blockquote> <h1 id="references">References</h1> <ol class="bibliography"><li><div class="row"> <div id="flowmodels2026" class="col-sm-12"> <span id="flowmodels2026">Holderrieth, P., &amp; Shprints, R. (2025). <i>MIT Computer Science Class 6.S184: Generative AI with Stochastic Differential Equations</i>. https://diffusion.csail.mit.edu/2025/index.html</span> </div> </div> </li> <li><div class="row"> <div id="lipman2022flow" class="col-sm-12"> <span id="lipman2022flow">Lipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., &amp; Le, M. (2022). Flow matching for generative modeling. <i>ArXiv Preprint ArXiv:2210.02747</i>.</span> </div> </div> </li> <li><div class="row"> <div id="liu2022flow" class="col-sm-12"> <span id="liu2022flow">Liu, X., Gong, C., &amp; Liu, Q. (2022). Flow straight and fast: Learning to generate and transfer data with rectified flow. <i>ArXiv Preprint ArXiv:2209.03003</i>.</span> </div> </div> </li> <li><div class="row"> <div id="sohl2015deep" class="col-sm-12"> <span id="sohl2015deep">Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., &amp; Ganguli, S. (2015). Deep unsupervised learning using nonequilibrium thermodynamics. <i>International Conference on Machine Learning</i>, 2256–2265.</span> </div> </div> </li> <li><div class="row"> <div id="song2020score" class="col-sm-12"> <span id="song2020score">Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., &amp; Poole, B. (2020). Score-based generative modeling through stochastic differential equations. <i>ArXiv Preprint ArXiv:2011.13456</i>.</span> </div> </div> </li> <li><div class="row"> <div id="ho2020denoising" class="col-sm-12"> <span id="ho2020denoising">Ho, J., Jain, A., &amp; Abbeel, P. (2020). Denoising diffusion probabilistic models. <i>Advances in Neural Information Processing Systems</i>, <i>33</i>, 6840–6851.</span> </div> </div> </li></ol>]]></content><author><name></name></author><category term="deep-generative-modeling"/><category term="diffusion"/><category term="flow"/><summary type="html"><![CDATA[This post covers the fundamentals of diffusion and flow matching models. The following notation and theoretical framework are primarily adapted from (Holderrieth &amp; Shprints, 2025).]]></summary></entry></feed>